---
title: Latest 5 Papers - March 04, 2025
labels: documentation
---
**Please check the [Github](https://github.com/dingyue772/DailyArxiv) page for a better reading experience and more papers.**

## Hallucination
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MedHallTune: An Instruction-Tuning Benchmark for Mitigating Medical Hallucination in Vision-Language Models](http://arxiv.org/abs/2502.20780v1)** | 2025-02-28 |  |
| **[Mitigating Hallucinations in Large Vision-Language Models by Adaptively Constraining Information Flow](http://arxiv.org/abs/2502.20750v1)** | 2025-02-28 | <details><summary>Accep...</summary><p>Accepted to AAAI 2025. Camera ready version</p></details> |
| **[CoMT: Chain-of-Medical-Thought Reduces Hallucination in Medical Report Generation](http://arxiv.org/abs/2406.11451v4)** | 2025-02-28 |  |
| **[Prompt-Guided Internal States for Hallucination Detection of Large Language Models](http://arxiv.org/abs/2411.04847v2)** | 2025-02-28 |  |
| **[Mitigating Object Hallucination in MLLMs via Data-augmented Phrase-level Alignment](http://arxiv.org/abs/2405.18654v3)** | 2025-02-28 | <details><summary>Publi...</summary><p>Published in ICLR 2025</p></details> |

## MLLM
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[New Dataset and Methods for Fine-Grained Compositional Referring Expression Comprehension via Specialist-MLLM Collaboration](http://arxiv.org/abs/2502.20104v2)** | 2025-02-28 |  |
| **[Mitigating Object Hallucination in MLLMs via Data-augmented Phrase-level Alignment](http://arxiv.org/abs/2405.18654v3)** | 2025-02-28 | <details><summary>Publi...</summary><p>Published in ICLR 2025</p></details> |
| **[AsymLoRA: Harmonizing Data Conflicts and Commonalities in MLLMs](http://arxiv.org/abs/2502.20035v1)** | 2025-02-27 |  |
| **[Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution](http://arxiv.org/abs/2409.12961v4)** | 2025-02-27 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025</p></details> |
| **[Improving Adversarial Transferability in MLLMs via Dynamic Vision-Language Alignment Attack](http://arxiv.org/abs/2502.19672v1)** | 2025-02-27 | <details><summary>arXiv...</summary><p>arXiv admin note: text overlap with arXiv:2403.09766</p></details> |

## Reasoning
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[LLM Post-Training: A Deep Dive into Reasoning Large Language Models](http://arxiv.org/abs/2502.21321v1)** | 2025-02-28 | <details><summary>31 pa...</summary><p>31 pages, 7 figures, 3 tables, 375 references</p></details> |
| **[Towards Developing Ethical Reasoners: Integrating Probabilistic Reasoning and Decision-Making for Complex AI Systems](http://arxiv.org/abs/2502.21250v1)** | 2025-02-28 |  |
| **[ARIES: Autonomous Reasoning with LLMs on Interactive Thought Graph Environments](http://arxiv.org/abs/2502.21208v1)** | 2025-02-28 |  |
| **[Cryptis: Cryptographic Reasoning in Separation Logic](http://arxiv.org/abs/2502.21156v1)** | 2025-02-28 |  |
| **[Spatial Reasoning with Denoising Models](http://arxiv.org/abs/2502.21075v1)** | 2025-02-28 | <details><summary>Proje...</summary><p>Project website: https://geometric-rl.mpi-inf.mpg.de/srm/</p></details> |

## Reward Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization](http://arxiv.org/abs/2406.11431v3)** | 2025-02-28 | <details><summary>Accep...</summary><p>Accepted at ICLR 2025, camera-ready version</p></details> |
| **[Reward Learning from Multiple Feedback Types](http://arxiv.org/abs/2502.21038v1)** | 2025-02-28 | <details><summary>Publi...</summary><p>Published as a conference paper at ICLR 2025</p></details> |
| **[Alleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimation](http://arxiv.org/abs/2502.19941v2)** | 2025-02-28 |  |
| **[Self-Evolved Reward Learning for LLMs](http://arxiv.org/abs/2411.00418v2)** | 2025-02-28 | <details><summary>23 pa...</summary><p>23 pages,6 figures,Accepted to ICLR 2025</p></details> |
| **[Multi-Turn Code Generation Through Single-Step Rewards](http://arxiv.org/abs/2502.20380v1)** | 2025-02-27 | <details><summary>9 pag...</summary><p>9 pages (not including references or appendix); 6 figures (in main paper); (v1) preprint</p></details> |

